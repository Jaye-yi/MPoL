.. _rml-intro-label:

======================================================
Introduction to Regularized Maximum Likelihood Imaging
======================================================

This document is an attempt to provide a whirlwind introduction to what Regularized Maximum Likelihood (RML) imaging is, and why you might want to use this MPoL package to perform it with your interferometric dataset. Of course, the field is rich, varied, and this short introduction couldn't possibly do justice to cover the entire field. We recommend that you check out many of the links and suggestions in this document for further reading.

Data in the Fourier domain
--------------------------

First, we'll introduce the type of interferometric astronomical data that MPoL is designed to work with. Currently, we are most focused on modeling datasets from radio interferometers like the `Atacama Large Millimeter Array <https://almascience.nrao.edu/>`__ (ALMA), so most of the terminology will follow that specific use case. But the idea of interferometric modeling is general, and the MPoL package could be equally applied to imaging problems involving Fourier data from optical and infrared telescopes (please get in touch, if this is your use case).

Intereferometers acquire data samples in the Fourier domain, also called the visibility domain. The visibility domain is the Fourier transform of the image sky brightness

.. math::

    {\cal V}(u,v) = \iint I(l,m) \exp \left \{- 2 \pi i (ul + vm) \right \} \, \mathrm{d}l\,\mathrm{d}m.


Acquire a collection of datapoints, contain real and imaginary values. Other instruments might only provide robust products like closure phase.

Complex-valued visibility measurements

.. math::

    \boldsymbol{V} = \{V_1, V_2, \ldots \}_{i=1}^N

Defined in the presence of noise

.. math::

    V_i = \mathcal{V}(u_i, v_i) + \epsilon

Where :math:`\epsilon` itself is a complex-valued noise draw. Alternatively, we could say that the real and imaginary components of the noise are separately generated by draws from normal distributions characterized by standard deviation :math:`\sigma`

.. math::

    \epsilon_\Re \sim \mathcal{N}(0, \sigma) \\
    \epsilon_\Im \sim \mathcal{N}(0, \sigma)

and

.. math::

    \epsilon = \epsilon_\Re + i \epsilon_\Im

Radio interferometers will commonly represent the standard deviation as a "weight" :math:`w` where the relationship between the two is

.. math::

    w = \frac{1}{\sigma^2}

Additional References
+++++++++++++++++++++

* Getting started in radio astronomy (links to textbooks, lecture materials, and good lectures)
* Essential radio astronomy
* TMS textbook
* NRAO summer school lectures


Inference with forward models
-----------------------------

Typically, when astronomers are fitting a model to some dataset :math:`\boldsymbol{D} = \{D_1, D_2, \ldots \}_{i=1}^N`, like a line to a series of :math:`x` and :math:`y` points, we require a likelihood function. This specifies the probability of the data, given the model.

Usually we say that we have a model :math:`M`, which has a set of parameters :math:`\boldsymbol{\theta}`. (In the line example, the parameters might be the slope and intercept of the line, e.g. :math:`\boldsymbol{\theta} = \{m, b\}`.)

The likelihood function is defined as :math:`p(\boldsymbol{D} |\,\boldsymbol{\theta})`, and is sometimes written as :math:`\mathcal{L}(\boldsymbol{D} |\,\boldsymbol{\theta})`. And then one way to "fit" a model to data is to find the model parameter values which maximize the likelihood function.

Our statement about the measurement process (:math:`V_i = \mathcal{V}(u_i, v_i) + \epsilon`) defines a likelihood function for complex visibility measurements :math:`\boldsymbol{V}`

.. math::

    \mathcal{L}(\boldsymbol{V} | \boldsymbol{\theta}) =


A side note that this type of parameter inference is entirely `possible with the MPoL package <https://github.com/MPoL-dev/MPoL/issues/33>`__. In fact, the gradient-based nature of things should make this very fast and use advanced sampler like Hamiltonian Monte Carlo.

RML images as non-parametric models
-----------------------------------

Examples of splines vs. polynomials.

What is RML imaging?
=
Is

What does that mean?


In general, we are working with Fourier datasets. Meaning that we are trying to reconstruct images of the sky, but the datasets we have are related to the Fourier transform of that.

These types of datasets appear in radio interferometry (such as with ALMA, the JVLA, or very long baseline arrays like the Event Horizon Telescope), or optical interferometry, such as with sparse aperture masking.

Some advantages to doing RML imaging. Provides an alternative to assessing image quality w/ tclean.

Essentially model fitting
Likelihood. Loss functions. (link). Different formulations between Bayesian probability and/or regularizer formulation. An excellent resource here is the EHT-IV paper.

All of this is in contrast to the CLEAN algorithm, which operates as an image-plane deconvolution algorithm.


Additional references for RML imaging
+++++++++++++++++++++++++++++++++++++

* Narayan and Nityananda
* EHT IV

Additional refereces for CLEAN imaging
++++++++++++++++++++++++++++++++++++++

* NRAO summer schools
* CASA documentation



The MPoL package for Regularized Maximum Likelihood imaging
-----------------------------------------------------------

What's new here? Autodifferentiation. Opportunities for expansion. And the tight integration with PyTorch and neural networks. Easy to run on the GPU (link)

2) Getting started with imaging (links to CASA, other imaging software)
3) Getting started with PyTorch


Existing RML packages. Encourage you to check out.

This package is meant to be modular.


### Introduction to Regularized Maximum Likelihood (RML) Imaging

Regularized Maximum Likelihood (RML) imaging is a forward modeling methodology. We predict an image then represent it as an array of pixels. This is brought to the visibility domain through a forward Fourier transform. This is then compared to the measured data to ensure it is a viable predicted image. Due to information loss of the true image when obtaining the measured data, several predicted images- including incorrect ones- will match. To get to our best predicted image, we make new predictions by choosing the most likely (Maximum Likelihood) configuration and favoring specified criteria (Regularized). These criteria or regularizers, are chosen by the user. Some examples of favored criteria are smoothness and sparsity. The likeliness and how well a predicted image meet a certain criterion is mathematically represented in a loss function that contains hyperparameters used to weight data and regularizers. We minimize this loss function by performing a gradient descent, in which we adjust the pixel value intensities. Within this optimization run, hyperparameters are usually held fixed, but can be tuned between runs to produce a better image. When the loss function is minimized, our predicted image is at its best version to fit the collected data and follow our specified criterion.



- Package for synthesis imaging and model fitting from interferometric data.
- Built on PyTorch provides state of the art autodifferentiation capabilities
- Well tested, stable, on supported Python versions. Always a goal of core, usable routines in PyPi releases (i.e., `pip install mpol`). Maintainability.
- Scalability. By keeping modules modular, and *open* and emphasizing the building of imaging components rather than a single, monolithic function, the interested user can expand their applications.

Show the example of HD 143006 CLEAN vs. RML as example of why you might want to use this packageâ€¦ resolution, sensitivity, independent characterization of interesting features.


These could be nice videos, but aspects of them probably need to be tutorials first.

 * Autodifferentiation
     * neural networks, deep learning, graident descent, JAX
 * Layers + Nodes w/in Neural landscape
    * Input and Output connections.
    * Relation of "loss" to Bayesian inference
 * RML Imaging as forward modeling
     * optimization as training

Following on from the layer discussion, and the relationship to Bayesian inference, the idea is that there is some set of parameters that maximize the posterior.


One approach would be to combine all of the data into a single container, and just train/optimize off of that.


But let's say you had a combination of multiple datasets, from different telescope and there was an unknown calibration factor for each telescope.


This approach would be to "batch" the data in the training loop, and train in each step. This training loop is commonly to other neural network architectures.
