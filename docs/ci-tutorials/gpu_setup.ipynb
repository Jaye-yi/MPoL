{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6607894a",
   "metadata": {},
   "source": [
    "# GPU Acceleration\n",
    "## Installation and Configuration\n",
    "### Installing CUDA Toolkit\n",
    "The first step in utilizing the computational power of your Nvidia GPU is to install the CUDA toolkit. (If you’ve already configured your GPU for other software, you may skip this step.) To download the installer, visit [this link](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_network) and provide your system info to download the installer. You must be using either Linux or Windows, and you must be using one of [these](https://developer.nvidia.com/cuda-gpus) graphics cards. Once the toolkit has been installed, follow the instructions in the installer GUI. Once complete, restart your computer.\n",
    "\n",
    "### Python and PyTorch GPU configuration\n",
    "The next step is to check whether your Python and PyTorch installations are correctly configured to use your GPU. After installing MPoL, you can check whether everything installed correctly by opening up a Python interpreter, ard running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3164106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483922ed",
   "metadata": {},
   "source": [
    "This command should return `True`. If not, then you may need to use a more specific installation process. Go to the [PyTorch Official Site](https://pytorch.org/) and scroll down on the page until you see the **Install PyTorch** section. Input your specifications for your needs into this area and use the text that is generated for your install. For example, making of this tutorial on a Windows 10 system with a Nvidia GTX 1080 required specific pip installation, while another Windows 10 system using a Nvidia GTX 1660Ti worked with the default `pip install torch torchvision`. Your mileage may vary.\n",
    "\n",
    "## Why use the GPU?\n",
    "Using a GPU can accelerate computing speeds up to 100x over CPUs, especially for operations on large images, like is common for MPoL. The following is a quick example showing the addition of two large vectors. Your exact timing may vary, but for our hardware this calculation took 320 milliseconds seconds on the CPU, while it only took 3.1 milliseconds on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f3a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06305766105651855\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "N = int(9.9e7)\n",
    "A = torch.ones(N)\n",
    "B = torch.ones(N)\n",
    "start = time.time()\n",
    "C = A + B\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ad701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # emptying the cache on the gpu just incase there was any memory left over from an old operation\n",
    "A = A.cuda()\n",
    "B = B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07714144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.944896221160889\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "C = A + B\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a03c28",
   "metadata": {},
   "source": [
    "## Using the GPU as part of PyTorch and MPoL\n",
    "Here is a short example demonstrating how to initialize an MPoL model and run it on the GPU. First we will set our device to the CUDA device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959e763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be760c",
   "metadata": {},
   "source": [
    "This if-else statement is used just to ensure that we aren’t trying to run PyTorch on the GPU if it isn’t available. The rest of this tutorial will assume that `device=cuda:0`.\n",
    "\n",
    "**Note**: `cuda:0` is technically only required if you have more than one GPU. `device='cuda'` will instruct PyTorch to use the default cuda device.\n",
    "\n",
    "Now that we have our device set, we’ll initialize the MPoL dataset as in previous tutorials. This example uses a multi-channel dataset, but for demonstration purposes we will only use the central channel (`central_chan=4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128f243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.utils.data import download_file\n",
    "import numpy as np\n",
    "from mpol import gridding, coordinates\n",
    "fname = download_file(\n",
    "    'https://zenodo.org/record/4498439/files/logo_cube.npz',\n",
    "    cache=True,\n",
    "    )\n",
    "d = np.load(fname)\n",
    "coords = coordinates.GridCoords(cell_size=0.03, npix=180)\n",
    "central_chan = 4\n",
    "gridder = gridding.Gridder(\n",
    "    coords=coords,\n",
    "    uu=d['uu'][central_chan],\n",
    "    vv=d['vv'][central_chan],\n",
    "    weight=d['weight'][central_chan],\n",
    "    data_re=d['data_re'][central_chan],\n",
    "    data_im=d['data_im'][central_chan],\n",
    ")\n",
    "dataset = gridder.to_pytorch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b01616",
   "metadata": {},
   "source": [
    "Next we’ll create a [`SimpleNet`](https://mpol-dev.github.io/MPoL/api.html#mpol.precomposed.SimpleNet) module to train to our data. For more detailed information, see the [Optimization Loop](https://mpol-dev.github.io/MPoL/ci-tutorials/optimization.html) tutorial or the MPoL SimpleNet [Source Code](https://mpol-dev.github.io/MPoL/_modules/mpol/precomposed.html#SimpleNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10eeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpol.precomposed import SimpleNet\n",
    "model = SimpleNet(coords=coords, nchan=dataset.nchan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4ee37",
   "metadata": {},
   "source": [
    "We are now ready to move our model and data to the GPU using the `tensor.to(device)` functionality common to most PyTorch objects. One can also use the `tensor.cuda()` to move the tensor to the default CUDA device. Both of these methods return a *copy* of the object on the GPU.\n",
    "\n",
    "We’ve borrowed a `config` dictionary from the [Cross Validation Tutorial](https://mpol-dev.github.io/MPoL/ci-tutorials/crossvalidation.html), which basically contains a set of parameters that resulted in a strong cross validation score for this particular dataset. For more details on these variables, see the [Cross Validation Tutorial](https://mpol-dev.github.io/MPoL/ci-tutorials/crossvalidation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621c4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dataset.to(device)\n",
    "model = model.cuda()\n",
    "config = {'lr':0.5, 'lambda_sparsity':1e-4, 'lambda_TV':1e-4, 'epochs':600}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a3665",
   "metadata": {},
   "source": [
    "We are now ready to train our network on the GPU. We will use a for-loop with 600 iterations (epochs) in which we will calculate the loss and step our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f638a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpol import losses\n",
    "\n",
    "# set the model to training mode\n",
    "model.train()\n",
    "for i in range(config['epochs']):\n",
    "    # set the model to zero grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    vis = model.forward()\n",
    "\n",
    "    # get skycube from our forward model\n",
    "    sky_cube = model.icube.sky_cube\n",
    "\n",
    "    # compute loss\n",
    "    loss = (\n",
    "        losses.nll_gridded(vis, dset)\n",
    "        + config['lambda_sparsity'] * losses.sparsity(sky_cube)\n",
    "        + config['lambda_TV'] * losses.TV_image(sky_cube))\n",
    "\n",
    "    # perform a backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d931b4",
   "metadata": {},
   "source": [
    "Congratulations! You have now trained a neural network on your GPU. In general, the process for running on the GPU is designed to be simple. Once your CUDA device has been set-up, the main changes to a CPU-only run are the steps requried moving the data and the model to the GPU for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89d164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
